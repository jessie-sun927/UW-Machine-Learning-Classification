## Week6 Topics:
* Evaluation metrics for classifiers
* Precision: Fraction of positive predictions that are actually positive
* Recall: Fraction of positive data predicted to be positive 
* Trade-off precision and recall by setting probability threshold
  - Pessimistic model: high precision, low recall
  - Optimistic model: low precision, high recall
* Choosing metric:
  - F1 measure, area-under-curve(AUC)
  - precision at k


## Algorithms: 
* Logistic regression


## Implementation Details of Programming Assignment:
Goal: The goal of this second notebook is to understand precision-recall in the context of classifiers.

 * Use Amazon review data in its entirety.
 * Train a logistic regression model.
 * Explore various evaluation metrics: accuracy, confusion matrix, precision, recall.
 * Explore how various metrics can be combined to produce a cost of making an error.
 * Explore precision and recall curves.
